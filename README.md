# Crypto Data Pipeline with Airflow & dbt

This project implements an automated, scalable data pipeline for cryptocurrency market data using Apache Airflow and dbt. It fetches data from the CoinGecko API, processes and transforms it with dbt models, and stores the results in your data warehouse for analysis and reporting.

---
![Pipeline Architecture](https://github.com/bao040/Crypto_airflow_dbt/blob/main/Slide1.jpg)
## ğŸš€ Project Overview

1. **Data Extraction**: Apache Airflow orchestrates scheduled tasks to pull raw market data (prices, volumes, market caps, etc.) from the [CoinGecko API](https://www.coingecko.com/en/api).
2. **Data Transformation**: dbt (data build tool) applies SQL-based transformationsâ€”staging, modeling, and testingâ€”to prepare clean, analytics-ready tables, and performing incremental materialization and snapshots in controlling how the data is treated.
3. **Data Storage**: The processed data is loaded into your target warehouse/schema (e.g., Snowflake, BigQuery, Postgres) for easy querying.
4. **Documentation & Lineage**: dbt Docs generates a browsable DAG and data lineage, so you can track dependencies and model metadata.

---

## ğŸ§± dbt Lineage & Medallion Architecture

The pipeline follows the **Medallion Architecture**, organizing data into three layers:

* **Bronze (raw)**: Ingested data from the CoinGecko API, including `raw.global_market`, `raw.coins_markets`, and `raw.coins_list`.
* **Silver (staging + intermediate)**: Cleaned and standardized using models like `stg_coins_markets`, `stg_coins_list`, `int_dim_time`, and `int_fact_market_metrics`.
* **Gold (business models)**: Analytics-ready tables such as `fact_market_metrics`, `fact_global_market_snapshot`, `dim_time`, and `dim_coins`.

The image below illustrates the data lineage automatically generated by dbt Docs:

![dbt Lineage](https://github.com/bao040/Crypto_airflow_dbt/blob/main/dbt_workflow.png)

This visual DAG helps you understand dependencies and how data flows from ingestion to final analytics tables.

---

## ğŸ› ï¸ Tech Stack

* **Apache Airflow**: Workflow orchestration and scheduling
* **dbt**: ELT transformations, testing, and documentation
* **CoinGecko API**: Source of cryptocurrency market data
* **Data Warehouse**: Snowflake / BigQuery / Postgres / ...
* **Docker & Docker Compose**: Containerized development and deployment
* **GitHub Actions** (optional): CI/CD for automated testing and deployments

---

## ğŸ“ Repository Structure

```text
â””â”€â”€ crypto_airflow_dbt
    â”œâ”€â”€ dags                  # Airflow DAG definitions
    â”‚   â””â”€â”€ coingecko_dag.py
    â”œâ”€â”€ dbt_airflow           # dbt project directory
    â”‚   â”œâ”€â”€ models            # dbt models (staging, marts)
    â”‚   â”œâ”€â”€ snapshots         # historical snapshots
    â”‚   â”œâ”€â”€ seeds             # seed CSV data
    â”‚   â”œâ”€â”€ macros            # dbt Jinja macros
    â”‚   â”œâ”€â”€ dbt_project.yml   # project config
    â”‚   â””â”€â”€ profiles.yml      # dbt connection profiles
    â”œâ”€â”€ docker-compose.yml   # Container orchestration
    â”œâ”€â”€ Dockerfile           # Airflow + dbt custom image
    â”œâ”€â”€ README.md            # Project documentation
    â””â”€â”€ requirements.txt     # Python dependencies
```

---

## âš™ï¸ Quickstart

1. Clone this repository:

   ```bash
   git clone https://github.com/YOUR_USERNAME/crypto_airflow_dbt.git
   cd crypto_airflow_dbt
   ```

2. Configure environment variables in `.env` or Airflow UI (e.g., `COINGECKO_API_KEY`, `DBT_PROFILE_TARGET`, `WAREHOUSE_URL`).

3. Start services via Docker Compose:

   ```bash
   docker-compose up -d
   ```

4. Initialize Airflow:

   ```bash
   docker-compose exec airflow-webserver airflow db init
   docker-compose exec airflow-webserver airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com
   ```

5. Access Airflow UI at `http://localhost:8080` and trigger the `coingecko_dag` DAG.

6. Run dbt models manually (optional):

   ```bash
   docker-compose exec airflow-scheduler dbt run --profiles-dir ./dbt_airflow
   ```

7. Generate dbt documentation:

   ```bash
   docker-compose exec airflow-scheduler dbt docs generate --profiles-dir ./dbt_airflow
   docker-compose exec airflow-scheduler dbt docs serve --port 8081
   ```

---

## âœ… Features

* **Automated Scheduling**: Hourly, daily, or customizable intervals via Airflow
* **Data Quality Tests**: dbt tests to ensure schema validity and data integrity
* **Incremental Models**: Efficient loading of new market data only
* **Snapshots**: Historical state capture for time-series analyses
* **Documentation**: Auto-generated DAGs and lineage via `dbt docs`

---

## ğŸ”® Future Enhancements

* Add **alerting** and **monitoring** for failed Airflow tasks.
* Integrate **CI/CD** with GitHub Actions for automated `dbt test` and `dbt run`.
* Extend with **Machine Learning** pipelines for price prediction.
* Support additional data sources (e.g., other exchanges, on-chain metrics).

---

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).

---

*Happy data modeling! ğŸš€*
